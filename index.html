<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Cheng Peng</title>

    <meta name="author" content="Cheng Peng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/x-icon" href="images/Personal/self.ico">

</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Cheng Peng「彭程」</name>
                                    </p>

                                    <p>
                                        I am a second-year Ph.D student in <a href="https://www.au.tsinghua.edu.cn/" target="_blank">Department of Automation, Tsinghua University</a>, , under the supervision of <a href="https://www.liuyebin.com/" target="_blank">Prof. Yebin Liu</a>.
                                    </p> 
                                    <p>
                                        Currently, I am a researcher intern in the ByteDance PICO, working on Digital Human Generation.
                                    </p> 

                                    <p>
                                        Email: chengpeng002[AT]gmail[DOT]com
                                    </p>

                                    <br>
                                    <p style="text-align:center">
                                        <!-- <a href="mailto:gaojw20@mails.tsinghua.edu.cn">Email@THU</a> &nbsp|&nbsp -->
                                        <!-- <a href="mailto:winstongu20@gmail.com">Email</a> &nbsp|&nbsp -->
                                        <a href="https://scholar.google.com/citations?user=3AEO_xUAAAAJ&hl=zh-CN">Google Scholar</a> 
                                        &nbsp|&nbsp
                                        <a href="https://github.com/pengc02">Github</a> 
                                        <!-- &nbsp|&nbsp -->
                                        <!-- <a href="https://twitter.com/WinstonGu_">Twitter</a> &nbsp -->
                                        <!-- <a href="https://www.zhihu.com/people/ji-e-yi-zhu-jia-93">Zhihu</a> &nbsp -->
                                    </p>
                                </td>
                                <td style="padding:3% 3% 3%;width:80%;max-width:80%">
                                    <a href="images/Personal/portrait.jpg"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/Personal/portrait.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <!-- <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research Interests</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->
<!-- 
                    <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <p>In the rapidly advancing and exciting field of robotics, 
                                        my research is focused on developing robot systems 
                                        that possess interpretability, robustness, agility, efficiency, 
                                        and maybe even further, with intelligence and grace. 
                                        I believe that integrating learning with control 
                                        as <strong>learning-based controllers</strong> is essential for realizing these traits. 
                                        Thererfore,  I seek to explore ways to utilizing the structural insights 
                                        of system dynamics provided by control theory, 
                                        to establish an optimized basis for learning algorithms.
                                    </p>
                                    <p>To go even further, 
                                        I believe that combining control and learning into a unified framework 
                                        will not only result in robust learning-based controllers but also <strong>revolutionize 
                                        people's perceptions of intelligence</strong>. 
                                        The intelligence of so called "embodied agents" in the future 
                                        might look similar to systems that perceive their environments 
                                        as inputs andgenerate corresponding outputs, similar to actions. 
                                        Moreover, principles such as stability, controllability, and observability 
                                        may become essential in analyzing how these agents 
                                        perceive environments, decide, store and retrieve memories, and take actions.
                                    </p>
                                    <p>Furthermore, 
                                        I am intrigued by the idea of applying <strong>physics-based character animation</strong> techniques to the learning process of humanoid robots.
                                        I think it would be very amusing to watch humanoid robots move and behave like animated characters from movies or video games.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->

                    <!-- Preprints -->
                   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                      <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                          <heading>Preprints</heading>
                        </td>
                      </tr>
                    </tbody>
                  </table>

                  <!-- FlexAvatar -->
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                      <tr></tr>
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                            <source src='pubs/2025.FlexAvatar/flexavatar-teaser.mp4'>
                          </video>
                        </div>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation</papertitle>
                        <br>
                        <strong>Cheng Peng</strong>*, Zhuo Su*, Liao Wang*, Chen Guo, Zhaohu Li, Chengjiang Long, Zheng Lv, Jingxiang Sun, Chenyangguang Zhang, Yebin Liu (*Equal Contribution)
                        <br>
                        <em>arXiv</em> 2025
                        <br>
                        <a href="https://pengc02.github.io/flexavatar">[Project]</a>
                        <a href="https://arxiv.org/abs/2512.17717">[PDF]</a>
                        <a href="pubs/2025.FlexAvatar/bibtex.txt">[BibTeX]</a>
                        <br>
                        <p>
                          We present FlexAvatar, a flexible large reconstruction model for high-fidelity 3D head avatars with detailed dynamic deformation from single or sparse images, without requiring camera poses or expression labels. 
                        </p>
                      </td>
                    </tbody>
                  </table>

                    <!-- Publications -->
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                        <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                          <heading>Publications</heading>
                        </td>
                      </tr>
                    </tbody></table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    
                    <!-- PGHM -->
                    <tr></tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                             <div class="one" style="position:relative;height:0;padding-bottom:100%;">
                                <img src="pubs/2025.PGHM/pipeline.jpg" 
                                    style="width:100%;height:auto;position:absolute;top:0;left:0;"
                                    alt="PGHM image">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling</papertitle>
                            <br>
                            <strong>Cheng Peng</strong>*, Jingxiang Sun*, Ruizhi Shao, Yuan-Chen Guo, Xiaochen Zhao, Yangguang Li, Yanpei Cao, Bo Zhang, Yebin Liu (*Equal Contribution)
                            <br>
                            <em>International Conference on 3D Vision</em>, 3DV 2026
                            <br>
                            <a href="https://pengc02.github.io/pghm/">[Project]</a>
                            <a href="https://arxiv.org/abs/2506.06645">[Arxiv]</a>
                            <!-- <a href="https://arxiv.org/abs/2410.12928">[Code]</a> -->
                              <!-- <a href="https://github.com/deepseek-ai/DreamCraft3D">[Code]</a> -->
                            <a href="pubs/2025.PGHM/bibtex.txt">[BibTeX]</a>
                            <br>
                            <p> We present the Parametric Gaussian Human Model (PGHM), a generalizable and efficient framework that integrates human priors into 3DGS for fast and high-fidelity avatar reconstruction from monocular videos. </p>
                        </td>
                    </tr>

                    <!-- HADES -->
                    <tr></tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                             <div class="one" style="position:relative;height:0;padding-bottom:100%;">
                                <img src="pubs/2025.HADES/hades.png" 
                                    style="width:100%;height:auto;position:absolute;top:0;left:0;"
                                    alt="HADES image">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>HADES: Human Avatar with Dynamic Explicit Hair Strands</papertitle>
                            <br>
                            Zhanfeng Liao, Hanzhang Tu, <strong>Cheng Peng</strong>, Hongwen Zhang, Boyao Zhou, and Yebin Liu
                            <br>
                            <em>International Conference on Computer Vision</em>, ICCV 2025
                            <br>
                            <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Liao_HADES_Human_Avatar_with_Dynamic_Explicit_Hair_Strands_ICCV_2025_paper.pdf">[Project]</a>
                            <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Liao_HADES_Human_Avatar_with_Dynamic_Explicit_Hair_Strands_ICCV_2025_paper.pdf">[Arxiv]</a>
                            <!-- <a href="https://arxiv.org/abs/2410.12928">[Code]</a> -->
                              <!-- <a href="https://github.com/deepseek-ai/DreamCraft3D">[Code]</a> -->
                            <a href="pubs/2024.DreamCraft3D_plus/bibtex.txt">[BibTeX]</a>
                            <br>
                            <p> We introduce HADES, the first framework to seamlessly integrate dynamic hair into human avatars. </p>
                        </td>
                    </tr>

                    <!-- DreamCraft3D_plus -->
                    <tr></tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one" >
                          <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                                      <source src='pubs/2024.DreamCraft3D_plus/demo_dc3d++.mp4'>
                          </video>
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>DreamCraft3D++: Efficient Hierarchical 3D Generation with Multi-Plane Reconstruction Model</papertitle>
                            <br>
                            Jingxiang Sun, <strong>Cheng Peng</strong>, Ruizhi Shao, Yuan-Chen Guo, Xiaochen Zhao, Yangguang Li, Yanpei Cao, Bo Zhang, Yebin Liu
                            <br>
                            <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, TPAMI 2025
                            <br>
                            <a href="https://dreamcraft3dplus.github.io/">[Project]</a>
                            <a href="https://arxiv.org/abs/2410.12928">[Arxiv]</a>
                            <!-- <a href="https://arxiv.org/abs/2410.12928">[Code]</a> -->
                              <!-- <a href="https://github.com/deepseek-ai/DreamCraft3D">[Code]</a> -->
                            <a href="pubs/2024.DreamCraft3D_plus/bibtex.txt">[BibTeX]</a>
                            <br>
                            <p> We present DreamCraft3D++, an extension of DreamCraft3D that enables efficient high-quality generation of complex 3D assets in 10 minutes. </p>
                        </td>
                    </tr>

                    <!-- Control4d -->
                    <tr></tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one" >
                        <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: 15%">
                                    <source src="pubs/2024.Control4D/teaser.mp4">
                        </video>
                            </div>
                        </td>

                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Control4D: Efficient 4D Portrait Editing with Text</papertitle>
                            <br>
                            Ruizhi Shao, Jingxiang Sun, <strong>Cheng Peng</strong>, Zerong Zheng, Boyao Zhou, Hongwen Zhang, Yebin Liu
                            <br>
                            <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2024
                            <br>
                            <a href="https://control4darxiv.github.io/">[Project]</a>
                            <a href="https://arxiv.org/abs/2305.20082">[Arxiv]</a>
                              <a href="https://github.com/threestudio-project/threestudio">[Code]</a>
                            <a href="pubs/2024.Control4d/bibtex.txt">[BibTeX]</a>
                            <br>
                            <p> We propose Control4D, an approach to high-fidelity and spatiotemporal-consistent 4D portrait editing with only text instructions. </p>
                        </td>
                    </tr>

                    </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Awards</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    Academic Excellence Award, Tsinghua University, 2021
                                    <br>
                                    <br>
                                    Comprehensive Excellence Award, Tsinghua University, 2022
                                    <br>
                                    <br>
                                    Outstanding Graduates (Beijing & Dept. of Automation, Tsinghua University), 2024
                                    <br>
                                    <br>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <!-- <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Experiences</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->


                    <!-- <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Presentations</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                        <tr>
                          <td style="padding:0px">
                            <br>
                            <p style="text-align:right;font-size:normal;">
                            Updated at Dec. 2025.
                            <br>
                            Template from <a href="https://jonbarron.info/" style="text-align:right;font-size:normal;" target="_blank">Jon Barron</a>. 
                            </p>
                          </td>
                        </tr>
                      </tbody>
                    </table>


                </td>
            </tr>
        </tbody>
        
    </table>
   

   
</body>

</html>